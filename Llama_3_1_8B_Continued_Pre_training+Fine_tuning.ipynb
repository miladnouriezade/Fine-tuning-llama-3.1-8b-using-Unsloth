{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipNK63ohvUuI"
   },
   "source": [
    "**Project Description**\n",
    "\n",
    "The objective of this project is to adapt a pre-trained LLM to a target domain (continued\n",
    "pre-training), and perform instruction fine-tuning on question-answering.\n",
    "\n",
    "💻 🧑 Developer: Milad Nourizade\n",
    "\n",
    "📧 E-mail: milad.nouriezade@gmail.com\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCVUE9N9Pymj"
   },
   "source": [
    "**Tool Selection**\n",
    "\n",
    "We are going to use **Unsloth** since we have a limited computational power and by far it's the most efficient framework for finetuning.\n",
    "[Unsloth](https://https://github.com/unslothai/unsloth?tab=readme-ov-file)\n",
    "is a free and open-source framework for fine-tuning and inferencing LLMs which focuses on single-GPU finetuning. It is 2.2x faster, uses 70% less VRAM, has 0% degradation in accuracy for QLoRA (4bit) and LoRA (16bit) finetuning. However, it has some drawbacks such as not supporting all the models available. Here is a [link](https://https://wandb.ai/augmxnt/train-bench/reports/Trainer-performance-comparison-torchtune-vs-axolotl-vs-Unsloth---Vmlldzo4MzU3NTAx) to a detailed comparison between popular fine-tuning tools `torchtune`, `axolotl` , and `Unsloth`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yjo1KETTn4e7"
   },
   "source": [
    "**Approach Overview**\n",
    "\n",
    "The final goal of this task is to have a domain adapted model that is addtionally finetuned for instruction following (instruction fine-tuning). This could be achieved by following steps:\n",
    "\n",
    "\n",
    "1.   Continued pretraining on the base model. The output of this step is the LoRA adapters.\n",
    "2.   Merge LoRA adapters with the base model and save the new adapted model with `16bit` precision.\n",
    "3.  Load the adapted model and quantize it with `4bit`.\n",
    "4.  Fine-tune for the instruction following and like steps `1` and `2` merge and save the final model.\n",
    "\n",
    "> Developed models are available at [huggingface.co/Dragonfluy](https://huggingface.co/Dragonfluy).\n",
    "\n",
    "\n",
    "> The major difference between  `fine-tuning` and `continued pretraining` is that for continued pretraining We also integrate LoRA adapters into `embed_tokens` and `lm_head` to allow the model to learn out of distribution data. Additionally, we should select a higher rank `r` for LoRA adapters to train more weights that leads to learning more complex structures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-Z36N0rcqom"
   },
   "source": [
    "**Model Selection**\n",
    "\n",
    "We select `Llama 3.1 8B` to start experimenting as a small base model since it has a high performance regarding to another proprietary models, long context size(`128k`) and appropriate size of parameters that meets our GPU memory limitation (Tesla T4). You can see the LLMs rank on each benchmark on [Lmarena](https://lmarena.ai/?leaderboard) or other leaderboards like [open_llm_leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) on Huggingface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9JpL9k32tRD"
   },
   "source": [
    "**Model Quantization**\n",
    "\n",
    "Model quantization is a greate startegy that hleps us to load a huge model into GPU meomory where we have a limited GPU like T4 and prevent OOM error. Additionally, it decrease the memory usage for limited VRAM and model download time, also inference is much faster. We applied `4bit quantization` which converts higher precision usually `32bit` weights to `4bit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eSvM9zX_2d3"
   },
   "outputs": [],
   "source": [
    "!pip install unsloth==2024.11.7\n",
    "# Also get the latest nightly Unsloth!\n",
    "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6jqTroXFk6M"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your actual Hugging Face API token string\n",
    "huggingface_token = \"Your huggingface token\"\n",
    "\n",
    "login(huggingface_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320,
     "referenced_widgets": [
      "3a7feb2d48224655a0251f2f6d1de74d",
      "fe08026f61b2445a8cc2ba4bc5062880",
      "da45ab26928a4912b79cd1aba938ca96",
      "1b317175af65462995d48c9d38de1319",
      "ef380d9acf0947e99861b8455ce9c283",
      "d75d9235c8314931bc63a04dfb9db17a",
      "86b6de27b16445bca55df3fd058be3e1",
      "fb48370325934d10af317f0a8cc8b90a",
      "54e49f8a8b25451dbb1ef088e86bcc4e",
      "0abb77b2ddda4f4ebfcff4ad80bf2cd3",
      "ca4bd1bf25684a00b72362cce157b69a",
      "ff0f69ab9a214b008a6dd7519e3fd837",
      "64c73a5d444b47cd9d48f3528037e43f",
      "fff4e3bc039b4ef3b873acd88297b36b",
      "18925278a35f49c5adacfc8b5ea14ed0",
      "8b60b7f3eafd414685eba06e6269764f",
      "78eab92bbe974387bd731561514ecf7f",
      "2a0ba9741be943209b2a4cff2256852e",
      "fdae29f29f814633ae4e41ea0d80d694",
      "30e0a3b156c64bfa88e6a1c4ac4ec489",
      "d74c65b5b27e4dbcb3bb90ee93e717e3",
      "3e33455c101f4902877eb4cc8effabe7",
      "00b3dd61cb614a4bb19d6de0b162853b",
      "d063545e93864293bdc0031047ddcea4",
      "d8c3bf2eef6f4322a87d0027c6741eaf",
      "9c783a85b9ef44e09327efebc75a4567",
      "b00160a630284213b15f7b9e6973c635",
      "82c97bea054a48ecbd34b76d29132f3a",
      "cb08a460ded94b8f8603192e47a6a8d2",
      "e71bb969ec824dbbb5b35a6cfef276b8",
      "7efd8315890944879f5db39edfe11cf9",
      "1993daf3bb634c2d81380142d33de38c",
      "08c81c731ac74e488ba72615d45d32bc",
      "1156d0916477460b9e3aeccf9f4f4821",
      "f29f504c28b5421b904107c7fadab6b3",
      "573508a8ae0e4053a5738df894e4bae8",
      "c9fd1460b42c435a845f40830cb8e863",
      "da24cf680a084a26ae2a3167dee3bd13",
      "4935a6979a404ec38c3eeb8444aac973",
      "1b48a22f42af4919b7bccd4d7a73b8c4",
      "6da5f4c9df2045d6b090c2e858907752",
      "d172539ea61948379d35ed1dee10310f",
      "6f0529ff66a7437d83214146a9b475f8",
      "d1692b2a1ccf43c0b3fe70caa7961a25",
      "ef3d6c29b3b446889f1cd11bbdafa945",
      "372c7a8efeeb4109a79f787e51c576fc",
      "712ca40daec542c9b5588662de3e8f24",
      "665a388e55a14c12925c3e5235809780",
      "569d2a00128a49bda8d2b59719597fb0",
      "0a29a82ccfce40eeb7d98740d5e739b5",
      "dc052796d4684a2082a85d7702074e2b",
      "c5104818ce83464d88a0867753209a5f",
      "7977d8905ced47969b4f7547c75b20f8",
      "4b64ed44029142a3bb6de6977822e670",
      "e0952ac8984544fb9fd1cdd433b40c0e"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "1cea14f1-efbe-4275-a645-9207e8f95d83"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "**Continued Pretraining**\n",
    "\n",
    "Using LoRA ( Low Rank Adaptor ) technique, we can fine tune a language model with much less parameters. As previously discussed, in addition to `Attention` and `Feed forward` layers we need to add both `embed_tokens` and `lm_head`, however since we have a limited memory we can remove `embed_tokens` otherwise we will get OOM error. Selection of the `r` and `lora_alpha` is depend on the model size and dataset complexity.\n",
    "\n",
    "> ⁉ \"LoRA r doesn't significantly impact the final performance as long as LoRA adapters are used on all layers of the model.\" This is an interesting finding from [QLORA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "5118427f-c901-4f35-8653-5c3172efb0e2"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"], # Add for continual pretraining\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "### Data Prepration\n",
    "Now using the astro abstracts dataset from https://huggingface.co/datasets/UniverseTBD/arxiv-astro-abstracts-all. For speeding up the process we only sample the first `5k` rows and devide it to `train` and `validation` set. We also must add `EOS_TOKEN` or `tokenizer.eos_token` or else the model's generation will go on forever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "a825be2c0c714028ab80ca4621d22f3c",
      "1fe047ef344145e083ba154da1cfefe7",
      "907c2fbff95f40868a6ec1788d02b2a0",
      "dc69a74c7e52416c9318302fd71d5079",
      "105db6b0060244fdb4a60adf8f8b42cb",
      "d5cf46109a334ae9a8be481784ab0ce0",
      "7ab802e2c02d4bc48735a1f5e1e6b3fe",
      "50e48deab58443d6b3a9847256b1e0c2",
      "e554b087037e417d8eee885a09c124d4",
      "9ea63f0e47da4d3bb5b2f538f7b3ef9d",
      "2df2ba006b5544ae9b89b3b656aad55f",
      "50f3ee8c715d4715a55430dee45bc3b1",
      "a939bf75ab1b466daf79d51ec7142b09",
      "f41052674b764396bd22b78b1f890b67",
      "fdd37f25908c4442a48b66081780350a",
      "cefc0dc99173414d91678de1ffad689e",
      "f405a23bec2e459db497d5568aae7051",
      "7dd967c4ca644071a33fdf2f9d3f7a8f",
      "530d7be836c14e4db3241601a0b7b61a",
      "d9767b26007c47428603243e74b6a29c",
      "a5324c6bb938482b87d990c5c651a3e7",
      "2a1de7b26ca04ccc95fb8e9ae0c703fa",
      "00669e62d7bb48c4b89c96d93d4214c3",
      "9652ffd43415430d84af2be765dee8e4",
      "ed11269c18844d59aa00e0e989db2c65",
      "48a3cdd373ee4691af85e618498bac17",
      "c4e3394b1666418dabc4ebc443fa2b5e",
      "e4517573e7db417483ed17cf160f1514",
      "f3e5722f64bf482eae2a5279fad8eedd",
      "493d436599c04a51926876a1e67f7c37",
      "5cf9c903c6624a9c9cfe4073a3fcb564",
      "861d74d71975439b80cf25c733394482",
      "71db770022a04fc6809c24a3d6fd6506",
      "d6d6fcc2a17d4320afecbea608871e38",
      "e362bf9e09334efd844e801fac0dae1c",
      "540d0cc9004a4f3e8abcfcc2eca94355",
      "66ab1656ddbf49adbd23db2e6294abe8",
      "bcaf0e1943754f7ea9b8bce86ff9feef",
      "d9a27293fd50403f923be73e3b483df0",
      "99c4959314994ed9a2fe01e4bab3e5bf",
      "bd78f34707f74287bf9f3fde7dc456d9",
      "b11c9da2057d4e4fbebd5141e95351f4",
      "618a0ab1ef3b4d90bb65ddf47ac9eefa",
      "06af21ae93b5404db7dc84edbe810377",
      "784464966a034c82953551679eeae4ce",
      "d67e7070165749a2bc801ee9caf8a5d9",
      "a6931eeeda954677a762e51be87598d1",
      "e4a6bd4ca9eb465f9509e14c212de9c9",
      "05906c5c690b4841b0b45c3b74e76bc7",
      "689a341038a14b2cb2456bc402d44303",
      "0c71e996f1ff40a89e8f3ba07b2c10fa",
      "b1da90bff9924086bca6108fb94e06dd",
      "b2cf83261ada4973b3ba54aaa0b4d219",
      "991ca14b29d7490893727b8d3e7b745f",
      "e932c54ee24246ae99d97fdf757268ab",
      "75d2b5e22d0e456a9834111163c9f429",
      "487ee9ef3a884d28b1a280acdb27a945",
      "993b491a7c054c5a9dad7b99ce48b711",
      "1b5e8e153ca5450db556a97447ecf855",
      "f1a683d88d2e4ae7b95cb83cce9f4ab1",
      "95c84bbb94e546e28ca85b644a1f4eba",
      "2892c8c9c1f64d3bbfdbb04357758f3a",
      "c8a8a60880134cce89851eaeee778415",
      "1d5b79c0bf9b4b4680643ce61fa15adb",
      "08aa8124aecf46ca9110917c3ff7c570",
      "81ea6338fc0d42d99f96b65936358f83",
      "be71b439b93f49fa8362c6cd8c770378",
      "d6c8e1c5991549e29481ceab682d2dd8",
      "caa6de97f56c49f9b816e389b8d4a6f8",
      "773ac7c730bc4988a9f9fc3f958581a9",
      "3929af37dc6f4e3eab0b1ef03a502334",
      "a156ddbc068b4f17a9a98915a516f5c3",
      "d73b50483b6149af8bfe4aab0833f767",
      "7af4eb8115b046fd9a6d2655031735c7",
      "c86bf5962f104ef2a1b100ea15026939",
      "285d641915b34432bd167d2c50999802",
      "98e2574a10a7412cadcd01e34b88d1fd",
      "f1826553e5734956aaf22dea9b1ecab7",
      "db73de23442240cba76421fbec00d630",
      "39c75bc533564e1c8bbad6ea4b866339",
      "dae9a54aec0f44c283d41d84ad7066a5",
      "18c93bc7261349cca3e854a54565ed67",
      "9b62e47cdc7d42d7ba98c221ed67ee4f",
      "698bb58ab93c4b9597da4cc0e0dc2b81",
      "1825c8371a284b589c941be3fd340d89",
      "4e73581bcec147fc888c4ccd70c74c8a",
      "acb5324f041548b285da2089b51e233c",
      "11d9b5881f7b458c97f5d272c65d5036"
     ]
    },
    "id": "yXt8Na97yRe7",
    "outputId": "431d46a5-f2a8-490b-d487-bca35d6a0c72"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"UniverseTBD/arxiv-astro-abstracts-all\", split = \"train[:5000]\")\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    return { \"text\" : [example + EOS_TOKEN for example in examples[\"text\"]] }\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "dataset_dict = dataset.train_test_split(test_size=0.05)\n",
    "\n",
    "train_dataset = dataset_dict['train']\n",
    "eval_dataset = dataset_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLnmCEyyHtjH",
    "outputId": "3a82ee97-9663-4158-9e8b-988c188e5931"
   },
   "outputs": [],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "### Continued Pretraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155,
     "referenced_widgets": [
      "e2668c2b04e74a9abc288339fa55f30f",
      "e0b1daa8a1c54383abfd39d0fd25e46d",
      "c8815b24692b493ba14353850001c1e1",
      "778786337c75453ba1e7f348a6984c60",
      "c81f95e7118d4842ae6d1305c72b8e38",
      "af23bee12e6f4c5ba88c7e844610dd53",
      "402fb50c7f1d4152a797c35b59d459a6",
      "8c4496d8d2604bdb94aae6501459d5ec",
      "9e3c307dc29f411a82a030337ce5a57b",
      "01b12048735b4dfc8ff4942fa1e3fa5e",
      "f9d6da48180c415a863e86bd1b5e9bf9",
      "2159cfd4345c4586a20b82e30fe37843",
      "7ff96565537148e7bb2d12160df07814",
      "19c5bcdec61a45af817f1da3e63ea71f",
      "1e5830ef6f534e13a35fa0c1755db19a",
      "9702d34b74c6455483ca5c10f716a230",
      "666a172fd2104cb5bb9e46dcb1de1f1d",
      "02048b0397354cc49d220f63f29b3ea5",
      "c836af5a6f60446a86034e72b2440aba",
      "24ea915c07924368bda455c0c28d64b5",
      "73cbfcf7db234326a8709712d7498ff7",
      "f92477d2dc8e4d4d9419889aa9bc7c6a"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "a95270de-7e39-4b8b-cff6-c622e760cb29"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 8,\n",
    "\n",
    "        # num_train_epochs = 1,\n",
    "        max_steps = 20,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        warmup_ratio=0.1,\n",
    "\n",
    "        learning_rate = 2e-5,\n",
    "        embedding_learning_rate = 1e-5, #set embedding_learning_rate to be a learning rate at least 2x or 10x smaller\n",
    "                                        #than learning_rate to make continual pretraining work!\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.00,\n",
    "        lr_scheduler_type = \"linear\", # We can experiment with \"cosine\" and \"constant\"\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tp5siycp22jX"
   },
   "source": [
    "The training and validation loss fluctuation showing that we need to put more time here to find appropriate hyperparameters combination. We can experiment with different `r`, `r_alpha`, `lr` or even increasing the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "fa4f139d-b0f2-400b-d0fd-b3d692bb3b5b"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "ede6e603-caa3-4250-a1b0-4054ef45ab53"
   },
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VajUWZ9XgCNv"
   },
   "source": [
    "**Save Model**\n",
    "\n",
    "Now it's time to merge the LoRA adapters with the base model to build the new model and finally upload it to huggingface profile which is available on this[link](https://huggingface.co/Dragonfluy/astro_adapted_llama_3.1_8b). To have a higher precision and preventing performance dropping we saved the model's weight with `16bit` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459,
     "referenced_widgets": [
      "3d0551dae5224970b2db23ab4f303b3b",
      "9c4fc503850944938e94c3c30b520b76",
      "151b0d18569f403aab153aa747bf79d4",
      "9fe06f52a1874007b9c89703dd7a99f3",
      "b77aa94175e745a6b939101c6872c2ce",
      "04d2cb45a9814e08ae3200bc4a4ffb3d",
      "1d1175d0eeaf434cb68525b86e75af32",
      "56187921d6d64b69bae1316c19a5b285",
      "da5b51a245e24c52945032309422a5e8",
      "eed850c02fd741bb9ed7699c061d8399",
      "cf1eeab4fd3b4f6a9c8b68817a14f438",
      "5feaba42bcb74bc9b282e4a767e905eb",
      "11e6d6bf3e9d4c8c9bef041531a53aac",
      "311e5fb7c2534411937a81518a04926a",
      "59314bfd3e244024b2cbcf0681ff831d",
      "5c6e4af891e1487a96839a320dd760ed",
      "a00ee28f313843a994fcf2450dd6ead8",
      "873d3694af2e4f889827d9df7cd01673",
      "4ffb558a918a44a2becb344305548ee0",
      "c7eed64a2ee9470daa26a18c7fc04e1c",
      "8e0431dde07443bf9af2e54fb3be1fe3",
      "e6da37df1c2a4df2bba9313e1b4ced72",
      "205586c8b0ef420ebc4c661f4d5d8398",
      "e22b62d3c2704f53831a08089f98b769",
      "4bc43aca1be6459cace59eeb9a86d17d",
      "804d79b04b094d9d953a564bb4d18349",
      "84eb4259c1e747b9a568cf5562c02cea",
      "56c9ded16e3041c888eb92eba899acd9",
      "9fd344a319594ae984e3ff163bfac899",
      "0ddab6ba826045d3b931e2985d345337",
      "51166b34504847d2a827617ac54a0021",
      "ede0c69ce92e4e899cde45423b5451db",
      "08381ea8e45b4019b310e6e0cd3501c4",
      "ad1a196475cc40fe9390520c3b9a5fc6",
      "cf73ed345f7048dd9aeb10867764b505",
      "5c5505eea83047a0a2045c635c129500",
      "895b5fc3132a4b079a895e61330e608e",
      "dc5c97ebc3cf45fc980bcd8877536949",
      "0abe86e12bdb437e97895dca549010f2",
      "acbbe2bec43f4907bc2d2290db059ca3",
      "9dc9a3da720549b5b85b701fad273ea0",
      "03103c736479408992928cd716000527",
      "a6e94f83004c404c9504b4bcac752d0a",
      "632d379806e044c8b96ced36281b788c",
      "aafb3fbd319e4fceb52e20a333312e76",
      "9fd48e5bb4e04d6eaf80360500ae701a",
      "f6e32d83555d4ffb87eb65e0bec8f749",
      "750e2c697aea4837bd794a7b393af7f6",
      "0ee39ec97d9b41cfa5c59b571b6b2dd4",
      "0081775ef88f4be8a7f01f2143bfc384",
      "a180e1aa60f74ef08ddace7f0d3b755f",
      "3f7cf5ed63474187b6ebacab6d11274a",
      "ef88cb4896f447b4962ab17d17281979",
      "89f460b405d04d738e8d45bcc797bf80",
      "09a0cb97728b4a94b006cd516034fd56",
      "8104acfe33b941ac8e231a10e2351e72",
      "6e4d7edbbcd04eb4b9d55cf7dc36ff90",
      "efb04ebdfe6f4d858ff73cffc088a3b4",
      "e9a216114cd64229baf0a3c9f2a21c9b",
      "64df0c8bb50845129389c54dd21f3667",
      "24ef4d38c36042d589edfd42ddb37994",
      "06139ba2f47b4f77ad62a129a98f4c00",
      "69bd6fb05c7b45a9b5a3690f572d21a8",
      "99ebacbbd0d44a729e7550770dd8a3c1",
      "a094ae8b7ddc423d86efe514086b20a9",
      "c2946b0de02d4813b4a47e2ad1168045"
     ]
    },
    "id": "2d610xqdOy03",
    "outputId": "69f67b16-6dd9-4f8e-e28c-ed13493f080f"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub_merged(\"Dragonfluy/astro_adapted_llama_3.1_8b\", tokenizer, save_method = \"merged_16bit\", token = huggingface_token)\n",
    "# model.save_pretrained_merged(\"Astro_Adapted_Model\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSwpm8D6kr_x"
   },
   "source": [
    "### Instruction Fine-tuning\n",
    "\n",
    "Now let's load our new model that has been created in the first phase(cotinued pretraining) to continue instruction fine tuning. As discussed we set `load_in_4bit` to True to quantize the model's weight from `16bit` to `4bit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512,
     "referenced_widgets": [
      "920b26c2a2ce4780a543bafd8cb7a0e6",
      "ab0f2a373a30452c95cfe555b5589cae",
      "58f4774f384446dd99d9ee14711d188b",
      "605890b0ca454136910b50c6c6c1a400",
      "8fc1e1e2791c4409b9c997091e820123",
      "2d5556e94ac7447fbe2790ef6fd7cd67",
      "047064d78cad4368b5b363b6e8b007ab",
      "22ac19a85f214b2cb0bd59612a0ffb28",
      "063595bb22f3440d84c42bde61e3919d",
      "f15db8c075f04a3eb94ab0c90e99d8ac",
      "a293be4edb02446895e1d22d6e4dac49",
      "7690912924754cd5bf899bddb18de271",
      "3cf9286faa2b47d69b6259b19c9de38d",
      "c2749b0deb654726b4102ab7df78412e",
      "8fc808bf810543ac8d7ab99a4c189398",
      "3b92e22004b142baa18d093062725615",
      "e939f307ecb34600ac8f93469f2459f2",
      "f4f6f2c07b694c2ca10ee28e7e169cf2",
      "675250d4b1dd42c88ad3ec132313af48",
      "8332d3da4f034135bd908a156863221f",
      "3f461761cd1d434fa13374c37865f710",
      "82d78afbc04d4e0ab7395a588c153037",
      "37af06bbefcb47c5accc320581f7b103",
      "0a83dfde7a9846e1a2996605275e8dc0",
      "f5ab4904b5b84df685778e4ed21fb323",
      "2a43538595944aadb8c84d571d80277b",
      "bafe18bac6d949e6a8c54157c7ea24bc",
      "f44c0322963a41d794014314e124970f",
      "9d7361c55bab45a0912d17270cbc82f6",
      "b8712c08bafd4e26a78cb4d140e4f816",
      "abd01a1709cd439fbb5f35aeade99d91",
      "c6aaa84e46e441b590203302e23abb2e",
      "e7e8ea6fc2384c0f88c7a09d6613bc4c",
      "d8ed05df33184323baf88bb5dc65e644",
      "3af8dd4084e243cca8a2403fba8d9709",
      "2d38191781cb44328f15342ddf2deb31",
      "ff5ff3e9d508481d865799055793f1d4",
      "07a0e16261904e878d7c5e32288c38e5",
      "46de2f61eb664f25bac80627909f7fd7",
      "17d954a45c4f4e9b824fb406c82656c2",
      "d1cafbd8b026444ea5ee398f5e4f1a89",
      "b11c6c6a9fea4aabbfdfe4a6fe23d89c",
      "85db4f83593c466f928bd194dc1a3f10",
      "ff92aeae03354819a590514f100f371c",
      "7f7e3529db5a4209aaa83a8884164eb8",
      "3ea55a9c6fed49da98ac9f947473265f",
      "68201932016c41558f05d605a4a3310a",
      "421afa63952244a3a82250da1aa7b23a",
      "86307f9861734e82be3f1f421888bf13",
      "80dedb76ce9745dcb13601bfdf219af4",
      "ff3c7e9d2ede4df88af7c2c402da0a6e",
      "859fa50103b849859df93a07adaf59be",
      "0fe1b656c87b430e95545d00374ba7f3",
      "8dbb73d02fee4385be9f7c114c58ebce",
      "8b8c03d3f3374324b6a34c0ed0235bb1",
      "0cb6e71bb87f4ee9a6ddda21ff5734b1",
      "b06312116c9a488981e925f7b1e43104",
      "337dcabcdd3d4d1d944a5a959ca42b19",
      "1d90a86ab4be4f0e86b1a96c22d9fa86",
      "f7898bd32dce4a288a8fd4cafe816913",
      "09f3144ca51845e8a958164bf2c30c3b",
      "7c7c83e4c3a24ae7bd84ee282d7b6ba3",
      "aceecf29c00a44eba780a99e8f5a496a",
      "c1798877fb9b420c9790f1fb6bbe8cbc",
      "51e787f24cfa4786848becd60baf3ad6",
      "83e7ff71c3c8432c92a3c9d8721a9d38",
      "4d006f47baae407ba7e95eac76810be1",
      "5520330c7d88432b891126c6d202f2ca",
      "cd22b8960cc7400fbb87d21da6c2f663",
      "2a071919f61240418a6b0b4bd187f7c4",
      "5319fb7cd6d94b28a2d68ad57a96fd9e",
      "fb991ad6ef7d48ebb9fd42cd1e6f018c",
      "263b8dfc26474f1e982056adb57fd40f",
      "61283cda6caa4801871396174b918953",
      "e4ecc3e196f2469da736081e668557b5",
      "cd1f260ade3449d792f78c549d35657b",
      "36816d4216c74dba90d6a89f3fd88ac6",
      "aa3b45d3bcf6499fa53221d94b1f3599",
      "39198892b9334c5fb046e9f042317058",
      "24a256abe8bd474dadbfcac38b074096",
      "ecfb457cc4fe4d06a191d314ba7dbd39",
      "71f1dde222094343ac807b371c37bb2f",
      "dfdc23701e5a4b739d8b02cf31d1613f",
      "d81007409d124b60b76bb4272d5db29a",
      "550925f4f082406a880873df4d301703",
      "d11aa05b7f574548b97cfce91712ab99",
      "c033ddeca1834d24a5094aa5f4b91806",
      "5e81a29168264693ac9229bf5ca08d27",
      "0098069af0ed4c2db8eb7e474492fecb",
      "5bc81c4730e84988af42d679e4650d60",
      "3619407e67aa4b4fac60c8ae43cdc155",
      "85469f94fb0140b2a60118fb664d9200",
      "c8925db294d34ae0b6158b0aabddf90d",
      "92441c9c504a4edc9b3a62b90ff0b021",
      "92321ec939364b92999cd3b5560697d6",
      "c19821c11679463cb8c8e1fa735ba4c7",
      "fd7c1859bad04ee0b19a306d0d9f75b2",
      "f82dbbf2a2814908b66a454dd449cd04",
      "fe0dcd057c0947979f4c4c507df5c502",
      "a61e45af62b54c3ebcd3ae663283f0b1",
      "425e8468554d4706bdc614b8c1235917",
      "d2d3b92cb2d14f0d907b4fd991d39daa",
      "e5bf85b760ec4a2eb452d3222f2b0181",
      "b5086298471c4018b62ddbabe0aa1e25",
      "340336fbfa4342baaf53bd28f111f920",
      "88b921629812494fae88dd90c1fbf25a",
      "b64ff9a1810341a3b3f8651ba4540f94",
      "c6dc406de20641d1869de5a1dedf8940",
      "67816922e2fa4992a087358d64ffc238",
      "ddfc04784c784ecaac96c101146c6446",
      "968fe1dd520f439992ff71b7cefec81f",
      "fd73bcc26bf7493396e7e5ea32f1a7ac",
      "a6d355424cae4e7c9c5104726b85a061",
      "621a89c6c30e42a1b8dc01cae478e267",
      "cdee67739beb46eb99f66b11896b6e57",
      "e96f2dc9881a4fb5b9720a9d667a77f1",
      "f040f1db658c48269fde7d0490ac5a7d",
      "56031d8d966946ea8a1c4ec77a8058fc",
      "605f346872f24374b3c32533b056b595",
      "9063a2c02c074ad0b6a6b99466103221",
      "e9a31ae1edda4e26abaa10605ce51400"
     ]
    },
    "id": "I_xrk_4mhEf-",
    "outputId": "decc1111-e3ee-46c0-ec92-47289d232b9e"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    " # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Dragonfluy/astro_adapted_llama_3.1_8b\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sXa5FLp7Ryz"
   },
   "source": [
    "As before for the highest performance we inject LoRA adapters into all the layers but the difference is that for instruction fine-tuning we should not include `lm_head` and `embed_token`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-D_qi8ivlRag",
    "outputId": "ec13d3e6-e88e-4fe4-afaa-a6292200331f"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"], # Add for continual pretraining\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byfcHxDNSxTn"
   },
   "source": [
    "#### Data Prepration\n",
    "\n",
    "In this step we are preparing the [dataset](https://huggingface.co/datasets/daven3/geosignal) for instruction fine-tuning. We need to put the `Instruction`, `Input` and `Output` in a proper prompt format to feed the model. We still need `EOS_TOKEN` to indicate the end of sequence. Finally, we split the dataset (first 5000 rows) into two parts `train` and `validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJVredU9qMJ_"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"daven3/geosignal\", split = \"train[:5000]\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "dataset_dict = dataset.train_test_split(test_size=0.005)\n",
    "\n",
    "train_dataset = dataset_dict['train']\n",
    "eval_dataset = dataset_dict['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CamA88kWwZ4P",
    "outputId": "c0437715-494e-4184-f3b6-0a2fc431c094"
   },
   "outputs": [],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190,
     "referenced_widgets": [
      "fa47c8f0686f4de59faf405b1667aff2",
      "37751e4c4bb7443aafab51035771be60",
      "ea12f91b96084b44be176785b2811841",
      "7c8a5dcb6c2d4abcb52407696d6a0472",
      "8ef91772314a478282ffcff220eb304b",
      "8fb63decac974c18bc6a39ac7826da3e",
      "53b1561303394361b7fb92d6be68475d",
      "8cf5c2b5b91e4826b0152b4ad28d5551",
      "30633c246740464fa8ca195898a1ba5d",
      "f9ee31637df349f98ad4e3885001953a",
      "989841fb687643ccb9142fcff248e70c",
      "7b9a563cc74142d2a7879cfae77bec3c",
      "b85e446e8b3a42898e66c3e39c372dda",
      "d08028526369474490ca00e064a3a473",
      "8aafe570ea2b4e3e8fac97c7afb94a33",
      "bd03d7d83baf43cfa39912bc49ccaa1a",
      "a6726b11b7b54abf875ed2f89fe462dc",
      "4e17c0be331d4f61a49da28d33a6b1f4",
      "e821cfaac1c24ef996c7cd959f56a0c6",
      "aa1b545349b74c08924b9bd4f4514125",
      "a4bb825e07cd4294865b1e56991fae21",
      "f4b2b7cccf2147c0a78e07227899970f"
     ]
    },
    "id": "JPwz2_rosKZ5",
    "outputId": "179c9796-9055-4f62-9bc6-4ab6a4f2816b"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = True, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        # eval_steps=5,\n",
    "        warmup_ratio=0.1,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPFn0uIv5be8"
   },
   "source": [
    "As the training process indicates, the `Validation Loss` and `Training Loss` are smoothly decreasing showing that the training process is going well. However, by monitoring loss we can not be sure how good a large language model would perform and we need a set of new metrics to evaluate our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 980
    },
    "id": "ecQ2lcLLsk_J",
    "outputId": "ee67a4c1-7179-44a6-86fe-46235e65bdbb"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584,
     "referenced_widgets": [
      "8aee0c4e1f3643b1baf057b61a1a62fd",
      "316fb08c087a494ea28a7aa587967c59",
      "e407b156af374c3c99ce66fcf10e8588",
      "da76e812111744f8abd2b30d88b03e2c",
      "b8b3bea68bab402a8956ae3698f24a0e",
      "5d73e067b58d4ead95190c43a295461d",
      "08b2a6068d684dd2af17162e5efc3f65",
      "45a8f7b679ed413597827d0236e83f80",
      "04d97c0e16a64062bb17883cef7de49a",
      "92390207f8f14ad6b93b56c7fa8fb8ef",
      "302bdd1e88cf40e986df538f756609f3",
      "56858a4092574535b7d64f8f629f7995",
      "18cccec67aa14904a11260891b0ca1f6",
      "f8e4f7567d344073974941675fbe3d43",
      "20aafe88aa1d4fb98a6baf29d4415382",
      "1ab1e8fb9b28429781bc27fbdd356af8",
      "3a5351f942a24a7cad32d27d3821160e",
      "716e9363b1004663b90d4fa7a8968c1a",
      "6fba26dc048f45b988f56a759a56ab5c",
      "6c9860a0ac8042e3994fea9448c99a9f",
      "9aef3e5d2ae34c898d028f7af886bb94",
      "5477bdd345554fa5bbc216770ef0de13",
      "2477a06839854038b5e09da3c0e391cd",
      "6a42c2b19a114c01a15b96fd1f98abb2",
      "d527c4905106490daab20eba8279fdc1",
      "f8de42ab5bef4b2e80a70325a598e58b",
      "b2d49dc3aedc43a0b4305d16da0ebf2d",
      "ae7f56cd48a34db4975b2d1077d8540f",
      "157f47bb64a14009aef6d77729e24d4b",
      "428ca3abbb0343c0bbf5d39bb854d655",
      "30441408ce7e430da1102060a58611f6",
      "6533a95e27be44ee88dea33a666c1388",
      "00fb8d2752ad44caab406e0b60c8ce75",
      "7d9e007ecc0a447e83bd123e9c07d9f2",
      "a73a38e5636149febef0ab735c447470",
      "187b04996fcd4ac89026e6c9d53d301f",
      "bddd97f7193c44108e4c0ed2765471b8",
      "f1ae4f9c74da48da996407eb2d3ffb45",
      "aaab8b5e8020465bb06b202756a1158f",
      "76ac05790e884938a21647da51e6fadf",
      "1b26b920cea14e45981aa1d5ca1b6a44",
      "3c71a3ad94944dda993e46272495f2c1",
      "dddce69b2065480897d91bba81338e04",
      "0f7826ce02234445866bacaff4f35203",
      "a1703043a4e1436ca5c65218aa963ebd",
      "bb283968ce6b4c7e89bb6f106c74e6d7",
      "94818e7811f74314a345a9547b893c22",
      "7064921078d04405a08bed0a81d339af",
      "a28be7491e0a4515ae8955e893aeea4d",
      "1aa29a252d19416dae23ec6a54c18312",
      "a70b15a02d3c462a8bbe01297df8dd3e",
      "eecb10b25f1348b9b3182fecfe0f510b",
      "0bf687cb174a4ef4962609cf2177a3f7",
      "c2cbe6d52a764af6bd6df0f18721e87c",
      "ad551cbfb56d405a90cc3abc9eaa13ee",
      "3aad45b6e2b24d248890ac91a8b2d6ec",
      "7612f5d679804777b18b0a5dfdd06e9e",
      "b25713e371e640a4b3f4e668a40d84b5",
      "2fd2169330c442699734c05d2145efd3",
      "9698f2990e844e35802a9f0add68cb9a",
      "4954707d07904dfeb525effa2a2e4470",
      "1df2fcaaa4be4faa8eb93f721577f99f",
      "d0ae3ba6cd7c48d9be185f4b48382b09",
      "6815bef3a53648aab08d118600e6238b",
      "1a1a5b804d814a269216a15cd5321ff4",
      "f81bf335572f4baaa1d77297323c218c"
     ]
    },
    "id": "B7kv8iQV9_da",
    "outputId": "ece1e14b-5454-49af-cd69-79534a280798"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub_merged(\"Dragonfluy/astro_instruct_llama_3.1_8b\", tokenizer, save_method = \"merged_16bit\", token = huggingface_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "### Inference\n",
    "Let's run the model!\n",
    "\n",
    "We first will try to see if the model follows the style and understands to write a abstract that is within the distribution of `UniverseTBD/arxiv-astro-abstracts-all`. We select \"A detailed analysis of Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI),\" from testset to continue writing.\n",
    "\n",
    "Next, we select an instruction from `daven3/geosignal` to see how the models performas on instruction tuning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHm11vaQRt1U",
    "outputId": "1386b2fb-1bd1-4763-95d2-849a8528e04a"
   },
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "text_streamer = TextIteratorStreamer(tokenizer)\n",
    "import textwrap\n",
    "max_print_width = 100\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    \"A detailed analysis of Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI),\"\n",
    "]*1, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    inputs,\n",
    "    streamer = text_streamer,\n",
    "    max_new_tokens = 512,\n",
    "    use_cache = True,\n",
    ")\n",
    "thread = Thread(target = model.generate, kwargs = generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "length = 0\n",
    "for j, new_text in enumerate(text_streamer):\n",
    "    if j == 0:\n",
    "        wrapped_text = textwrap.wrap(new_text, width = max_print_width)\n",
    "        length = len(wrapped_text[-1])\n",
    "        wrapped_text = \"\\n\".join(wrapped_text)\n",
    "        print(wrapped_text, end = \"\")\n",
    "    else:\n",
    "        length += len(new_text)\n",
    "        if length >= max_print_width:\n",
    "            length = 0\n",
    "            print()\n",
    "        print(new_text, end = \"\")\n",
    "    pass\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nM5yZAblD6Zs",
    "outputId": "74fd656c-1cfc-4c2e-d414-6833e0b5c5d1"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)  # Enable faster inference\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            \"Give me a bulleted list of the past 10 Masters Tournament Champions.\",  # instruction\n",
    "            \"\",  # input\n",
    "            \"\",  # output - leave this blank for generation!\n",
    "        )\n",
    "    ], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "outputs = tokenizer.decode(outputs[0])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNS7IvTa72cJ"
   },
   "source": [
    "### Further Improvement\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "One of the imprtant missing part in the process of finetuning a large language model is evaluation. Considering the time constraint, we had to skip this step but it's vital to do evaluation after each finetuning.\n",
    "\n",
    "[lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/main?tab=readme-ov-file) is a open source framework for evaluating LLMs on different task and general benchmarks.\n",
    "\n",
    "**Perplxity**\n",
    "\n",
    "Perplexity is a critical metric used to evaluate the performance of language models, especially in domain adaptation scenarios. It quantifies how well a probability distribution predicts a sample. Lower Perplexity shows that model has predicted the sequence with higher accuracy.\n",
    "\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Considering the low training time (20-30 steps), limited input data model performed well on instruction fine-tuning. Having more compute unit let us to include `embed_token` for LoRA adapters, experiment with a higher rank( `r`) and loading more data to achieve a better result regarding the continued-pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V5X-htF1RnK"
   },
   "source": [
    "Future studies:\n",
    "\n",
    "* [AstroLLaMA: Towards Specialized Foundation Models in Astronomy](https://arxiv.org/abs/2309.06126)\n",
    "* [AstroMLab 3: Achieving GPT-4o Level Performance in Astronomy with a Specialized 8B-Parameter Large Language Model](https://arxiv.org/html/2411.09012v1)\n",
    "*   [QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning](https://https://arxiv.org/abs/2402.10462)\n",
    "* [Can AI Understand Our Universe? Test of Fine-Tuning GPT by Astrophysical Data](//https://arxiv.org/abs/2401.02981)\n",
    "* [Designing an Evaluation Framework for Large Language Models in Astronomy Research](https://https://arxiv.org/html/2405.20389v1)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
